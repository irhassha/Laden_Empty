import streamlit as st
import pandas as pd
import requests
import base64
import json
import io
import time
from PIL import Image

# --- KONFIGURASI HALAMAN ---
st.set_page_config(
    layout="wide", 
    page_title="RBM Auto Tally",
    page_icon="âš“"
)

# --- CSS CUSTOM ---
st.markdown("""
<style>
    .main > div { padding-top: 2rem; }
    div[data-testid="metric-container"] {
        background-color: #f8f9fa;
        border: 1px solid #e0e0e0;
        padding: 15px;
        border-radius: 8px;
    }
</style>
""", unsafe_allow_html=True)

# --- SESSION STATE ---
if 'extracted_data' not in st.session_state: st.session_state['extracted_data'] = []
if 'images' not in st.session_state: st.session_state['images'] = {}

# --- FUNGSI UTILITY ---
@st.cache_data(ttl=300) 
def get_prioritized_models(api_key):
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}"
    try:
        response = requests.get(url)
        if response.status_code == 200:
            data = response.json()
            all_models = [m['name'].replace('models/', '') for m in data.get('models', []) if 'generateContent' in m.get('supportedGenerationMethods', [])]
            if not all_models: return []
            flash = [m for m in all_models if 'flash' in m.lower()]
            pro = [m for m in all_models if 'pro' in m.lower() and m not in flash]
            return flash + pro + [m for m in all_models if m not in flash and m not in pro]
        return []
    except: return []

def render_image_viewer(df, key_suffix):
    st.markdown("---")
    c1, c2 = st.columns([1, 2])
    with c1:
        st.info("ðŸ” **Cek Gambar Asli**")
        vessel_list = df['Vessel'].unique() if not df.empty else []
        if len(vessel_list) > 0:
            selected_vessel = st.selectbox("Pilih Kapal:", vessel_list, key=f"v_{key_suffix}")
            selected_id = df[df['Vessel'] == selected_vessel].iloc[0]['NO']
        else:
            selected_id = None
    with c2:
        if selected_id and selected_id in st.session_state['images']:
            st.image(st.session_state['images'][selected_id], caption="Dokumen Asli", use_container_width=True)
    st.markdown("---")

# --- SIDEBAR ---
with st.sidebar:
    st.title("âš™ï¸ Config")
    if "GEMINI_API_KEY" in st.secrets:
        api_key = st.secrets["GEMINI_API_KEY"]
        st.success("âœ… API Connected")
    else:
        api_key = st.text_input("Gemini API Key", type="password")
    
    if api_key: 
        api_key = api_key.strip()
        with st.spinner("Checking..."): 
            active_models = get_prioritized_models(api_key)
        if not active_models: st.error("Invalid Key")
            
    st.divider()
    if st.button("ðŸ—‘ï¸ Reset Data", use_container_width=True):
        st.session_state['extracted_data'] = []
        st.session_state['images'] = {}
        st.rerun()
    st.info("v2.5: Auto-Retry Logic Added")

# --- HEADER ---
st.title("âš“ RBM Auto Tally")
st.markdown("Automasi ekstraksi data operasional pelabuhan (Anti-Limit & High Accuracy).")
st.divider()

# --- INPUT ---
st.subheader("Upload Laporan")
uploaded_files = st.file_uploader("Upload Gambar", type=['png', 'jpg', 'jpeg'], accept_multiple_files=True)

# --- CORE EXTRACTION WITH AUTO-RETRY ---
def extract_with_retry(image, api_key, max_retries=3):
    """
    Fungsi wrapper untuk menangani Error 429 (Limit) dengan melakukan retry otomatis.
    """
    if image.mode != 'RGB': image = image.convert('RGB')
    buffered = io.BytesIO()
    image.save(buffered, format="JPEG")
    img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    
    models = get_prioritized_models(api_key)
    if not models: models = ["gemini-1.5-flash"]

    prompt = """
    Analisis gambar tabel operasi pelabuhan ini. Butuh data granular.
    Baris: FULL, REEFER, OOG, DG/IMO, T/S FULL, T/S DG, T/S EMPTY, T/S OOG, EMPTY.
    Kolom: DISCHARGE(20,40,45), LOADING(20,40,45), SHIFTING(20,40,45).
    
    RULES:
    1. DG: Ekstrak ke field *_dg.
    2. HATCH COVER: Jika > 200, anggap 0.
    3. Kosong = 0.
    
    JSON Output (snake_case):
    {
        "imp_20_full": int, "imp_20_dg": int, "imp_20_reefer": int, "imp_20_oog": int, "imp_20_ts_full": int, "imp_20_ts_dg": int, "imp_20_ts_empty": int, "imp_20_ts_oog": int, "imp_20_empty": int,
        "imp_40_full": int, "imp_40_dg": int, "imp_40_reefer": int, "imp_40_oog": int, "imp_40_ts_full": int, "imp_40_ts_dg": int, "imp_40_ts_empty": int, "imp_40_ts_oog": int, "imp_40_empty": int,
        "imp_45_full": int, "imp_45_dg": int, "imp_45_reefer": int, "imp_45_oog": int, "imp_45_ts_full": int, "imp_45_ts_dg": int, "imp_45_ts_empty": int, "imp_45_ts_oog": int, "imp_45_empty": int,
        "exp_20_full": int, "exp_20_dg": int, "exp_20_reefer": int, "exp_20_oog": int, "exp_20_ts_full": int, "exp_20_ts_dg": int, "exp_20_ts_empty": int, "exp_20_ts_oog": int, "exp_20_empty": int,
        "exp_40_full": int, "exp_40_dg": int, "exp_40_reefer": int, "exp_40_oog": int, "exp_40_ts_full": int, "exp_40_ts_dg": int, "exp_40_ts_empty": int, "exp_40_ts_oog": int, "exp_40_empty": int,
        "exp_45_full": int, "exp_45_dg": int, "exp_45_reefer": int, "exp_45_oog": int, "exp_45_ts_full": int, "exp_45_ts_dg": int, "exp_45_ts_empty": int, "exp_45_ts_oog": int, "exp_45_empty": int,
        "shift_20_full": int, "shift_20_reefer": int, "shift_20_empty": int,
        "shift_40_full": int, "shift_40_reefer": int, "shift_40_empty": int,
        "shift_45_full": int, "shift_45_reefer": int, "shift_45_empty": int,
        "hatch_cover": int
    }
    """
    
    payload = {"contents": [{"parts": [{"text": prompt}, {"inline_data": {"mime_type": "image/jpeg", "data": img_str}}]}], "generationConfig": {"response_mime_type": "application/json"}}
    
    # --- RETRY LOGIC ---
    for attempt in range(max_retries):
        for model in models:
            if "experimental" in model: continue
            
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
            try:
                response = requests.post(url, headers={'Content-Type': 'application/json'}, data=json.dumps(payload))
                
                if response.status_code == 200:
                    clean = response.json()['candidates'][0]['content']['parts'][0]['text'].replace('```json','').replace('```','').strip()
                    return json.loads(clean)
                
                elif response.status_code == 429:
                    # KENA LIMIT -> Tunggu dan Retry
                    wait_time = (attempt + 1) * 10 # Tunggu 10s, 20s, 30s...
                    st.toast(f"â³ Limit tercapai. Menunggu {wait_time} detik...", icon="âš ï¸")
                    time.sleep(wait_time)
                    break # Break inner loop to retry outer loop (or switch model)
                
                elif response.status_code >= 500:
                    continue # Try next model
                    
            except Exception:
                continue
                
    st.error("Gagal setelah beberapa kali percobaan. Mohon upload ulang sebagian.")
    return None

# --- PROCESS BUTTON ---
if st.button("ðŸš€ Mulai Proses", type="primary", use_container_width=True):
    if not uploaded_files or not api_key:
        st.warning("Lengkapi data dulu.")
    else:
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, file in enumerate(uploaded_files):
            status_text.caption(f"Memproses {i+1}/{len(uploaded_files)}: {file.name}...")
            img = Image.open(file)
            data = extract_with_retry(img, api_key)
            
            if data:
                # --- CALCULATIONS ---
                # Import
                imp_l_20 = sum([data.get(k,0) for k in ['imp_20_full','imp_20_dg','imp_20_reefer','imp_20_oog']])
                imp_l_40 = sum([data.get(k,0) for k in ['imp_40_full','imp_40_dg','imp_40_reefer','imp_40_oog']])
                imp_l_45 = sum([data.get(k,0) for k in ['imp_45_full','imp_45_dg','imp_45_reefer','imp_45_oog']])
                imp_e_20 = data.get('imp_20_empty',0); imp_e_40 = data.get('imp_40_empty',0); imp_e_45 = data.get('imp_45_empty',0)
                
                # Export
                exp_l_20 = sum([data.get(k,0) for k in ['exp_20_full','exp_20_dg','exp_20_reefer','exp_20_oog']])
                exp_l_40 = sum([data.get(k,0) for k in ['exp_40_full','exp_40_dg','exp_40_reefer','exp_40_oog']])
                exp_l_45 = sum([data.get(k,0) for k in ['exp_45_full','exp_45_dg','exp_45_reefer','exp_45_oog']])
                exp_e_20 = data.get('exp_20_empty',0); exp_e_40 = data.get('exp_40_empty',0); exp_e_45 = data.get('exp_45_empty',0)

                # TS (Separate)
                ts_l_20 = sum([data.get(f'{x}_20_ts_{y}',0) for x in ['imp','exp'] for y in ['full','dg','oog']])
                ts_l_40 = sum([data.get(f'{x}_40_ts_{y}',0) for x in ['imp','exp'] for y in ['full','dg','oog']])
                ts_l_45 = sum([data.get(f'{x}_45_ts_{y}',0) for x in ['imp','exp'] for y in ['full','dg','oog']])
                ts_e_20 = data.get('imp_20_ts_empty',0)+data.get('exp_20_ts_empty',0)
                ts_e_40 = data.get('imp_40_ts_empty',0)+data.get('exp_40_ts_empty',0)
                ts_e_45 = data.get('imp_45_ts_empty',0)+data.get('exp_45_ts_empty',0)

                # Totals
                teus_imp = (imp_l_20+imp_e_20)*1 + (imp_l_40+imp_e_40)*2 + (imp_l_45+imp_e_45)*2.25
                teus_exp = (exp_l_20+exp_e_20)*1 + (exp_l_40+exp_e_40)*2 + (exp_l_45+exp_e_45)*2.25
                teus_ts = (ts_l_20+ts_e_20)*1 + (ts_l_40+ts_e_40)*2 + (ts_l_45+ts_e_45)*2.25
                
                s_20 = data.get('shift_20_full',0)+data.get('shift_20_reefer',0)+data.get('shift_20_empty',0)
                s_40 = data.get('shift_40_full',0)+data.get('shift_40_reefer',0)+data.get('shift_40_empty',0)
                s_45 = data.get('shift_45_full',0)+data.get('shift_45_reefer',0)+data.get('shift_45_empty',0)
                teus_shift = s_20*1 + s_40*2 + s_45*2.25
                
                hatch = 0 if data.get('hatch_cover',0) > 200 else data.get('hatch_cover',0)
                
                d_id = len(st.session_state['extracted_data']) + 1
                row = {
                    "NO": d_id, "Vessel": f"Vessel ({d_id})", "Service Name": "-", "Remark": 0,
                    "IMP_LADEN_20": imp_l_20, "IMP_LADEN_40": imp_l_40, "IMP_LADEN_45": imp_l_45,
                    "IMP_EMPTY_20": imp_e_20, "IMP_EMPTY_40": imp_e_40, "IMP_EMPTY_45": imp_e_45,
                    "TOTAL BOX IMPORT": imp_l_20+imp_l_40+imp_l_45+imp_e_20+imp_e_40+imp_e_45, "TEUS IMPORT": teus_imp,
                    "EXP_LADEN_20": exp_l_20, "EXP_LADEN_40": exp_l_40, "EXP_LADEN_45": exp_l_45,
                    "EXP_EMPTY_20": exp_e_20, "EXP_EMPTY_40": exp_e_40, "EXP_EMPTY_45": exp_e_45,
                    "TOTAL BOX EXPORT": exp_l_20+exp_l_40+exp_l_45+exp_e_20+exp_e_40+exp_e_45, "TEUS EXPORT": teus_exp,
                    "TS_LADEN_20": ts_l_20, "TS_LADEN_40": ts_l_40, "TS_LADEN_45": ts_l_45,
                    "TS_EMPTY_20": ts_e_20, "TS_EMPTY_40": ts_e_40, "TS_EMPTY_45": ts_e_45,
                    "TOTAL BOX T/S": ts_l_20+ts_l_40+ts_l_45+ts_e_20+ts_e_40+ts_e_45, "TEUS T/S": teus_ts,
                    "TOTAL BOX SHIFTING": s_20+s_40+s_45, "TEUS SHIFTING": teus_shift,
                    "Total (Boxes)": (imp_l_20+imp_l_40+imp_l_45+imp_e_20+imp_e_40+imp_e_45) + (exp_l_20+exp_l_40+exp_l_45+exp_e_20+exp_e_40+exp_e_45) + (ts_l_20+ts_l_40+ts_l_45+ts_e_20+ts_e_40+ts_e_45) + (s_20+s_40+s_45),
                    "Total Teus": teus_imp+teus_exp+teus_ts+teus_shift
                }
                
                # RECON MAPPING (Merge logic applied above in Summary, kept granular here)
                # Helper to map fields easily
                def get_raw(prefix, size, type_): return data.get(f'{prefix}_{size}_{type_}', 0)
                
                # Populate granular fields
                for act, prefix in [("IMP", "imp"), ("EXP", "exp")]:
                    for sz in [20, 40, 45]:
                        row[f"{act}_{sz}_Full"] = get_raw(prefix, sz, "full") + get_raw(prefix, sz, "dg") # Merge Visual Only? Or Keep Separate?
                        # Note: User asked to merge DG to Full in Recon tab too? 
                        # "di tab recon gabungkan saja dengan data full" -> YES.
                        
                        row[f"{act}_{sz}_Reefer"] = get_raw(prefix, sz, "reefer")
                        row[f"{act}_{sz}_OOG"] = get_raw(prefix, sz, "oog")
                        row[f"{act}_{sz}_TS_Full"] = get_raw(prefix, sz, "ts_full")
                        row[f"{act}_{sz}_TS_DG"] = get_raw(prefix, sz, "ts_dg") # T/S DG KEPT SEPARATE
                        row[f"{act}_{sz}_TS_Reefer"] = 0 # Not in standard form usually
                        row[f"{act}_{sz}_TS_OOG"] = get_raw(prefix, sz, "ts_oog")
                        row[f"{act}_{sz}_TS_Empty"] = get_raw(prefix, sz, "ts_empty")
                        row[f"{act}_{sz}_Empty"] = get_raw(prefix, sz, "empty")
                        row[f"{act}_{sz}_LCL"] = 0

                # Shifting
                for sz in [20, 40, 45]:
                    row[f"SHIFT_{sz}_Full"] = get_raw("shift", sz, "full")
                    row[f"SHIFT_{sz}_Reefer"] = get_raw("shift", sz, "reefer")
                    row[f"SHIFT_{sz}_Empty"] = get_raw("shift", sz, "empty")
                    # Fill others with 0
                    for t in ["OOG", "TS_Full", "TS_Reefer", "TS_OOG", "TS_DG", "TS_Empty", "LCL"]:
                        row[f"SHIFT_{sz}_{t}"] = 0
                
                row["Hatch Cover"] = hatch
                
                st.session_state['extracted_data'].append(row)
                st.session_state['images'][d_id] = img
                
            progress_bar.progress((i+1)/len(uploaded_files))
        
        status_text.success("Selesai!")
        time.sleep(1)
        st.rerun()

# --- DISPLAY ---
if st.session_state['extracted_data']:
    df = pd.DataFrame(st.session_state['extracted_data'])
    
    tab1, tab2, tab3 = st.tabs(["Summary", "Recon", "Combine"])
    
    summary_cols = [c for c in df.columns if c.isupper() and c != "NO"] 
    summary_cols = ["NO", "Vessel", "Service Name", "Remark"] + [c for c in summary_cols if c not in ["NO", "Vessel", "Service Name", "Remark"]]
    
    with tab1:
        render_image_viewer(df, "t1")
        edited = st.data_editor(df[summary_cols], num_rows="dynamic", use_container_width=True)
        # Export logic... (simplified for brevity, same as before)
        
    with tab2:
        render_image_viewer(df, "t2")
        # Smart Filter Logic
        recon_cols_base = ["Vessel", "Service Name"] + [c for c in df.columns if c not in summary_cols]
        if "Hatch Cover" in recon_cols_base: 
            recon_cols_base.remove("Hatch Cover"); recon_cols_base.append("Hatch Cover")
            
        hide = st.checkbox("Sembunyikan 0", value=True)
        if hide:
            valid = [c for c in recon_cols_base if c in ["Vessel","Service Name"] or df[c].sum()!=0]
            st.data_editor(df[valid], num_rows="dynamic")
        else:
            st.data_editor(df[recon_cols_base], num_rows="dynamic")
            
    with tab3:
        # Combine logic (same as before)
        opts = df['NO'].tolist()
        sels = st.multiselect("Pilih:", opts, format_func=lambda x: f"{x} - {df[df['NO']==x]['Vessel'].iloc[0]}")
        if sels:
            sub = df[df['NO'].isin(sels)]
            nums = sub.select_dtypes('number').columns
            tots = sub[[c for c in nums if c not in ['NO','Remark']]].sum()
            comb = pd.DataFrame([tots])
            comb.insert(0, "Vessel", "COMBINED")
            st.dataframe(comb)
